import math

# Данные
intervals = [(48, 61.5), (61.5, 75), (75, 88.5), (88.5, 102), (102, 115.5), 
             (115.5, 129), (129, 142.5), (142.5, 156)]
m1 = [6, 13, 36, 55, 47, 24, 16, 3]
m2 = [7, 23, 51, 73, 89, 34, 21, 2]
alpha = 0.2

# Общее количество элементов в выборках
n1 = sum(m1)
n2 = sum(m2)

# Функции эмпирического распределения
F1 = []
F2 = []
cumulative_sum1 = 0
cumulative_sum2 = 0

for f1, f2 in zip(m1, m2):
    cumulative_sum1 += f1
    cumulative_sum2 += f2
    F1.append(cumulative_sum1 / n1)
    F2.append(cumulative_sum2 / n2)

# Максимальное расстояние между эмпирическими функциями распределения
D = max(abs(f1 - f2) for f1, f2 in zip(F1, F2))

# Критическое значение для уровня значимости alpha=0.2
c_alpha = 1.07  # Значение константы для alpha=0.2
D_critical = c_alpha * math.sqrt((n1 + n2) / (n1 * n2))

# Вывод результатов
print(f"Эмпирическая функция распределения для первой выборки: {F1}")
print(f"Эмпирическая функция распределения для второй выборки: {F2}")
print(f"Максимальное расстояние между эмпирическими функциями распределения: {D:.4f}")
print(f"Критическое значение D_critical: {D_critical:.4f}")

if D < D_critical:
    print("Гипотеза о том, что распределения одинаковы, не отвергается.")
else:
    print("Гипотеза о том, что распределения одинаковы, отвергается.")
